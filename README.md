# ğŸ” LLM-Powered Financial Analyst â€” A RAG System for Earnings Call Q&A

This project is an **industry-grade Retrieval-Augmented Generation (RAG)** pipeline built to intelligently answer user questions from **company earnings call transcripts**. It handles both **document-level** and **topic-level** questions by classifying them using an LLM agent, retrieves relevant chunks using multiple search strategies, and generates accurate, faithful answers.

---

## ğŸš€ Features

- ğŸ“„ **Handles both topic-level and document-level questions**
  - Classifies questions using an LLM-based agent
  - Generates answers either by summarizing the full document or retrieving relevant chunks

- ğŸ” **Multiple retrieval strategies**
  - Supports **MMR**, **exact match**, and **hybrid** search
  - Compatible with **OpenAI**, **BGE**, and other embedding models

- ğŸ§  **Modular and extensible RAG pipeline**
  - Built with **LangChain**, supports plug-and-play chains and prompts
  - Clean code structure and logging using Python best practices

- ğŸ“ˆ **Robust evaluation framework**
  - Uses **RAGAS** to evaluate metrics like **faithfulness**, **context recall**, and **answer relevance**

---

## ğŸ“‚ Project Structure

rag-financial-qa/
â”œâ”€â”€ ingestion/ # Data loading and cleaning
â”‚ â””â”€â”€ load_and_clean_pdf.py
â”‚ â””â”€â”€ split_docs.py
â”œâ”€â”€ prompts/ # Prompt templates for different tasks
â”‚ â””â”€â”€ question_classifier_prompt.py
â”‚ â””â”€â”€ response_prompt.py
â”‚ â””â”€â”€ doc_summary.py
â”œâ”€â”€ retriever/ # Retrieval and RAG chain integration
â”‚ â””â”€â”€ create_retriever.py
â”‚ â””â”€â”€ integrate_chains.py
â”œâ”€â”€ indexing/ # Embedding and vector store setup (e.g., Pinecone)
â”‚ â””â”€â”€ create_index.py
â”œâ”€â”€ evaluation/ # RAG evaluation using RAGAS
â”‚ â””â”€â”€ evaluate_rag.py
â”œâ”€â”€ main.py # Main script to run the pipeline
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

## ğŸ“Š Evaluation with RAGAS

The project includes an evaluation pipeline using [RAGAS](https://github.com/explodinggradients/ragas) to assess the quality of generated answers based on key metrics:

- **Context Recall**: Checks whether relevant context was retrieved from the document.
- **Context Precision**: measures the signal-to-noise ratio of the retrieved context
- **Faithfulness**: Measures if the answer is grounded in the retrieved context.
- **Answer Relevance**: Evaluates how relevant the generated answer is to the user question.

### ğŸ§± Tech Stack (Markdown Format)
- **LangChain** for orchestration

- **Pinecone** as vector store

- **OpenAI / Hugging Face** LLMs and embedding models

- **RAGAS** for evaluation

- Python **logging, modular pipeline**, unit testing

Reference- https://medium.com/data-science/evaluating-rag-applications-with-ragas-81d67b0ee31a
